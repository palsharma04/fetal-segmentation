# -*- coding: utf-8 -*-
"""Copy of MONAI_testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-VfqNfU8ixBdzttR_YlZCaDPEXhm8412

# Testing
"""
"""# **2D MONAI**"""
import glob
from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, ToTensord
from monai.data import CacheDataset, DataLoader
from monai.transforms import ScaleIntensityd
import os
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from monai.inferers import sliding_window_inference
from monai.metrics import DiceMetric
from monai.transforms import AsDiscrete, Compose
from monai.networks.utils import one_hot
from monai.transforms import Lambdad

def combine_fetus_labels(label):
    label = label.clone()
    label[label == 5] = 1
    return label

from monai.utils import first

# Step 1: Load test files
test_images = sorted(glob.glob("/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/testing_plots/imagesTs/*.nii.gz"))
test_labels = sorted(glob.glob("/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/testing_plots/labelsTs/*.nii.gz"))
test_files = []
for img, lbl in zip(test_images, test_labels):
    case_name = os.path.basename(img).split(".")[0]
    test_files.append({"image": img, "label": lbl, "case_name": case_name})

degree_min = -2.05
degree_max = 2.05
target_size = [128, 128, 128]
# Step 2: Define transforms
test_transforms = Compose([
    LoadImaged(keys=["image", "label"]),
    EnsureChannelFirstd(keys=["image", "label"]),
    Lambdad(keys="label", func=combine_fetus_labels),
    Spacingd(keys=["image", "label"], pixdim=(0.8, 0.8, 4.0), mode=("bilinear", "nearest")),
    SpatialPadd(keys=["image", "label"], spatial_size=target_size, method="symmetric"),
    Resized(keys=["image", "label"], spatial_size=target_size, mode=("trilinear", "nearest")),
    ScaleIntensityRanged(keys=["image"], a_min=0, a_max=1002, b_min=0.0, b_max=1.0, clip=True),
    ToTensord(keys=["image", "label"]),
])

# Step 3: Create dataset and loader
test_volume_ds = CacheDataset(data=test_files, transform=test_transforms)
test_check_loader = DataLoader(test_volume_ds, batch_size=1)
test_check_data = first(test_check_loader)
print("first volume's shape: ", test_check_data["image"].shape, test_check_data["label"].shape)

print(f"Test set ready: {len(test_check_loader)} samples")

from monai.data import PatchIterd, GridPatchDataset
from monai.transforms import SqueezeDimd

patch_func = PatchIterd(
    keys=["image", "label"],
    patch_size=(128, 128, 1),  # dynamic first two dimensions
    start_pos=(0, 0, 0)
)
patch_transform = Compose(
    [
        SqueezeDimd(keys=["image", "label"], dim=-1),  # squeeze the last dim
        Resized(keys=["image", "label"], spatial_size=[128, 128]),
    ]
)

test_patch_ds = GridPatchDataset(
    data=test_volume_ds, patch_iter=patch_func, transform=patch_transform, with_coordinates=False)
test_loader = DataLoader(
    test_patch_ds,
    batch_size=5,
    num_workers=0,
    pin_memory=torch.cuda.is_available(),
)
test_check_data = first(test_loader)

import torch
from collections import defaultdict

# After loading test file paths:
test_images = sorted(glob.glob("/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/testing_plots/imagesTs/*.nii.gz"))
test_labels = sorted(glob.glob("/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/testing_plots/labelsTs/*.nii.gz"))

case_names = [os.path.basename(img).split(".")[0] for img in test_images]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = UNet(
    spatial_dims=2,
    in_channels=1,
    out_channels=5,
    channels=(32, 64, 128, 256, 512),
    strides=(2,2,2,2),
    kernel_size=3,
    up_kernel_size=3,
    num_res_units=1,
    act='PRELU',
    norm='INSTANCE',
    dropout=0.5
).to(device)

data_dir = '/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/'
model.load_state_dict(torch.load(os.path.join(data_dir, "2d_best_metric_model.pth")))
model.eval()

# --- Step 2: Metrics
dice_metric = DiceMetric(include_background=False, reduction="mean", get_not_nans=False)
dice_metric_case = DiceMetric(include_background=False, reduction="mean", get_not_nans=False)
slice_dice_per_case = defaultdict(list)

# --- Step 3: Inference
all_preds = []
all_labels = []
case_dice_scores = []
all_case_names = []

num_classes = 5
roi_size = (128, 128)

with torch.no_grad():
    for batch in tqdm(test_loader):
        image = batch["image"].to(device)
        label = batch["label"].to(device)

        case_name = batch["case_name"][0]
        all_case_names.append(case_name)

        test_outputs = model(image)
        test_outputs = torch.argmax(test_outputs, dim=1).unsqueeze(1)
        test_outputs = one_hot(test_outputs, num_classes=num_classes)

        if label.ndim == 3:
            label = label.unsqueeze(1)
        label = one_hot(label, num_classes=num_classes)

        # Optional: track dice per slice
        dice_metric_case.reset()
        dice_metric_case(y_pred=test_outputs, y=label)
        slice_dice = dice_metric_case.aggregate().item()
        slice_dice_per_case[case_name].append(slice_dice)

        all_preds.append(test_outputs.cpu())
        all_labels.append(label.cpu())

# After inference: Average slices per case
case_dice_scores = []
final_case_names = []

for case, slice_scores in slice_dice_per_case.items():
    avg_case_dice = sum(slice_scores) / len(slice_scores)
    case_dice_scores.append(avg_case_dice)
    final_case_names.append(case)

import numpy as np
import pandas as pd

voxel_spacing_mm = [0.8, 0.8, 4.0]  # Adjust if needed
voxel_volume_mm3 = np.prod(voxel_spacing_mm)
voxel_volume_cc = voxel_volume_mm3 / 1000

label_names = {
    1: "fetus",
    2: "placenta",
    3: "umbilical_cord",
    4: "amniotic_fluid"
}

# Dict to store per-slice metrics by case
metrics_by_case = defaultdict(lambda: defaultdict(list))

for i in range(len(all_preds)):
    pred = all_preds[i].argmax(dim=1).squeeze().numpy()  # (slices, H, W)
    gt = all_labels[i].argmax(dim=1).squeeze().numpy()

    current_case = all_case_names[i]

    gt[gt == 5] = 1

    for label, name in label_names.items():
        pred_mask = (pred == label)
        gt_mask = (gt == label)

        tp = np.logical_and(pred_mask, gt_mask).sum()
        fp = np.logical_and(pred_mask, ~gt_mask).sum()
        fn = np.logical_and(~pred_mask, gt_mask).sum()

        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 1.0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 1.0

        pred_vol_cc = pred_mask.sum() * voxel_volume_cc
        gt_vol_cc = gt_mask.sum() * voxel_volume_cc
        abs_diff = pred_vol_cc - gt_vol_cc
        rel_diff = (abs_diff / gt_vol_cc) * 100 if gt_vol_cc > 0 else np.nan

        metrics_by_case[current_case][f"{name}_dice"].append(dice)
        metrics_by_case[current_case][f"{name}_precision"].append(precision)
        metrics_by_case[current_case][f"{name}_recall"].append(recall)
        metrics_by_case[current_case][f"{name}_abs_vol_diff_cc"].append(abs_diff)
        metrics_by_case[current_case][f"{name}_rel_vol_diff_percent"].append(rel_diff)

# --- Average metrics per case
final_results = []
for case, metrics in metrics_by_case.items():
    row = {"Case": case}
    for metric_name, values in metrics.items():
        values = [v for v in values if not np.isnan(v)]
        row[metric_name] = round(np.mean(values), 4) if values else np.nan
    final_results.append(row)

# --- Save CSV
df_final = pd.DataFrame(final_results)
save_path = os.path.join(data_dir, "testing_plots", "2d-all_metrics_per_case.csv")
df_final.to_csv(save_path, index=False)

print(f"Averaged slice-wise metrics saved to: {save_path}")
df_final.head()

from collections import defaultdict
import numpy as np
import os
import pandas as pd

voxel_volume_cc = np.prod([0.8, 0.8, 4.0]) / 1000  # update if spacing differs

label_names = {1: "fetus", 2: "placenta", 3: "umbilical_cord", 4: "amniotic_fluid"}

# gather per-slice metrics and voxel counts
case_stats = defaultdict(lambda: defaultdict(list))
case_vox   = defaultdict(lambda: defaultdict(lambda: {'pred':0, 'gt':0}))

for i in range(len(all_preds)):
    pred = all_preds[i].argmax(dim=1).squeeze().numpy()
    gt   = all_labels[i].argmax(dim=1).squeeze().numpy()
    gt[gt == 5] = 1                          # merge head & body
    case_id = all_case_names[i]

    for lab, name in label_names.items():
        pm = pred == lab
        gm = gt   == lab

        tp = np.logical_and(pm, gm).sum()
        fp = np.logical_and(pm, ~gm).sum()
        fn = np.logical_and(~pm, gm).sum()

        #pixel-wise metrics, keep slice averages
        dice      = 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn) else 1.0
        precision = tp/(tp+fp)        if (tp+fp)        else 1.0
        recall    = tp/(tp+fn)        if (tp+fn)        else 1.0

        case_stats[case_id][f"{name}_dice"     ].append(dice)
        case_stats[case_id][f"{name}_precision"].append(precision)
        case_stats[case_id][f"{name}_recall"   ].append(recall)

        # voxel tallies for volume stats
        case_vox[case_id][name]['pred'] += pm.sum()
        case_vox[case_id][name]['gt'  ] += gm.sum()

# aggregate to case level
rows = []
for case_id, slice_metrics in case_stats.items():
    row = {'Case': case_id}

    # pixel-wise metrics → slice mean
    for m, vals in slice_metrics.items():
        row[m] = round(float(np.mean(vals)), 4)

    # volume metrics → compute once per case
    for name in label_names.values():
        gt_vox   = case_vox[case_id][name]['gt']
        pred_vox = case_vox[case_id][name]['pred']
        gt_vol   = gt_vox   * voxel_volume_cc
        pred_vol = pred_vox * voxel_volume_cc
        abs_diff = pred_vol - gt_vol
        rel_diff = (abs_diff / gt_vol * 100) if gt_vol else np.nan

        row[f"{name}_gt_volume_cc"        ] = round(gt_vol,   2)
        row[f"{name}_pred_volume_cc"      ] = round(pred_vol, 2)
        row[f"{name}_abs_vol_diff_cc"     ] = round(abs_diff, 2)
        row[f"{name}_rel_vol_diff_percent"] = round(rel_diff, 2)

    rows.append(row)

df_final = pd.DataFrame(rows)
out_csv  = os.path.join(data_dir, "testing_plots", "2d-all_metrics_per_case.csv")
df_final.to_csv(out_csv, index=False)
print("Saved:", out_csv)
