# -*- coding: utf-8 -*-
"""3dm-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o0u08Y65v6AKOPUaJGKs8iQebNFffYg8

Mount Drive into Google Colab
"""

from google.colab import drive
drive.mount('/content/drive/')
# drive.mount("/content/drive/", force_remount=True)

"""Install MONAI"""

# Commented out IPython magic to ensure Python compatibility.
# Install monai
!pip install -q "monai-weekly[nibabel, tqdm, einops]"
!python -c "import matplotlib" || pip install -q matplotlib
# %matplotlib inline

"""Import Libraries"""

# Import libraries
import os
import shutil
import tempfile
import json
import random
from glob import glob

import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import nibabel as nib

from monai.losses import DiceCELoss, TverskyLoss
from monai.inferers import sliding_window_inference
from monai.transforms import (
    AsDiscrete,
    Activations,
    EnsureChannelFirstd,
    Compose,
    CropForegroundd,
    LoadImaged,
    Orientationd,
    RandFlipd,
    RandCropByPosNegLabeld,
    RandShiftIntensityd,
    ScaleIntensityRanged,
    ScaleIntensity,
    ScaleIntensityd,
    Spacingd,
    SpatialPad,
    SpatialPadd,
    RandRotate90d,
    RandBiasFieldd,
    RandAdjustContrastd,
    RandGaussianNoised,
    ResizeWithPadOrCropd,
    RandAffined,
    ToTensord,
    Resize,
    Resized,
    EnsureTyped,
    AsDiscreted,
    RandGaussianSharpend,
    RandFlipd,
    RandRotated,
    RandZoomd,
    Rand3DElasticd,
    RandSpatialCropd,
    CenterSpatialCropd,
    Lambdad

)

from monai.config import print_config
from monai.metrics import DiceMetric
from monai.networks.nets import UNet

from monai.data import (
    DataLoader,
    CacheDataset,
    load_decathlon_datalist,
    decollate_batch,
)

import torch
# print_config()


# switch off warning messages
import warnings
warnings.filterwarnings("ignore")

"""Create .json File"""

import os
import json
import random
from glob import glob

# Set random seed for reproducibility
random.seed(42)

# Define paths
image_dir = "/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/mibirth_training/imagesTr/"
label_dir = "/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/mibirth_training/refined-labelsTr/"

# Get sorted lists of images and labels
image_files = sorted(glob(os.path.join(image_dir, "*.nii.gz")))
label_files = sorted(glob(os.path.join(label_dir, "*.nii.gz")))

print(len(image_files))
print(len(label_files))

# Ensure correct pairing
assert len(image_files) == len(label_files), "Mismatch between image and label counts."

# Extract just the filenames (without full paths)
image_names = [os.path.basename(img) for img in image_files]

# Pair images and labels
data_pairs = [{"image": f"mibirth_training/imagesTr/{img}", "label": f"mibirth_training/refined-labelsTr/{img}"} for img in image_names]

# Shuffle dataset
random.shuffle(data_pairs)

# Split dataset: 80% train, 20% validation
num_total = len(data_pairs)
num_train = int(0.8 * num_total)
num_val = num_total - num_train  # Remaining as validation

train_data = data_pairs[:num_train]
val_data = data_pairs[num_train:]

# Create JSON structure
dataset_json = {
    "description": "3D MRI Fetal Segmentation Dataset",
    "modality": {"0": "MRI"},
    "name": "fetal_mri_dataset",
    "numTraining": len(train_data),
    "numValidation": len(val_data),
    "reference": "KCL",
    "release": "1.0",
    "tensorImageSize": "3D",
    "labels": {
        "0": "background",
        "1": "class_1",
        "2": "class_2",
        "3": "class_3",
        "4": "class_4",
        "5": "class_5"
    },
    "training": train_data,
    "validation": val_data
}

# Save JSON file
json_path = "/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/dataset.json"
with open(json_path, "w") as json_file:
    json.dump(dataset_json, json_file, indent=4)

print(f"✅ Dataset JSON saved to {json_path}")

"""Split dataset into training and validation sets"""

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define paths
data_dir = '/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/'
split_JSON = "dataset.json"
datasets = os.path.join(data_dir, split_JSON)

# Load dataset
train_files = load_decathlon_datalist(datasets, True, "training")
val_files = load_decathlon_datalist(datasets, True, "validation")

"""Since MONAI requires uniform image sizes, let's check the min/max dimensions of the dataset:"""

import nibabel as nib
import numpy as np
from glob import glob

# Define folder paths as strings
image_folder = "/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/mibirth_training/imagesTr/"
label_folder = "/content/drive/My Drive/BEng/Year3/Research_Project/MONAI_training/mibirth-segmentation/mibirth_training/refined-labelsTr/"

# Load all images
image_files = sorted(glob(image_folder + "*.nii.gz"))

# Get image shapes
shapes = [nib.load(f).shape for f in image_files]

# Find min/max shape
min_shape = np.min(shapes, axis=0)
max_shape = np.max(shapes, axis=0)

print(f"Minimum Image Shape: {min_shape}") # [436 544 94]
print(f"Maximum Image Shape: {max_shape}") # [436 544 135]

# images are different sizes so had to resize everything to make sure it runs smoothly

"""# Pre-processing

Resize Images to (128, 128, 128)

Normalize all images to a smaller size (recommended for deep learning training):
"""

# find the target shape (normalize to deep learning suitable size)
image_files = sorted(glob(image_folder + "*.nii.gz"))
shapes = [nib.load(f).shape for f in image_files]
target_size = [128, 128, 128]

def combine_fetus_labels(label):
    label = label.clone()  # Avoid changing in-place
    label[label == 5] = 1  # Set all '5' to '1'
    return label

# rotation degree (Rad): minimum and maximum rotation in radians for data augmentation
degree_min = -2.05
degree_max = 2.05

# define training data transformations
train_transforms = Compose([
    # load NIfTI images and labels from file paths into dictionaries with "image" and "label" keys
    LoadImaged(keys=["image", "label"]),
    # ensures the input image and label have the channel as the first dimension: [C, H, W, D]
    EnsureChannelFirstd(keys=["image", "label"]),
    Lambdad(keys="label", func=combine_fetus_labels), # combine fetus head and body
    # resample both image and label to a common voxel spacing: 0.8 mm x 0.8 mm x 4.0 mm
    Spacingd(keys=["image", "label"], pixdim=(0.8, 0.8, 4.0), mode=("bilinear", "nearest")),
    # symmetrically pad images and labels to reach the `target_size` before resizing
    SpatialPadd(keys=["image", "label"], spatial_size=target_size, method="symmetric"),
    # resize both image and label to the target 3D shape
    Resized(keys=["image", "label"], spatial_size=target_size, mode=("trilinear", "nearest")),
    # normalize the intensity values of the image to [0, 1] or similar range
    ScaleIntensityd(keys=["image"]),
    # data augmentatin: apply random affine transformations (rotation, scale, etc.) with 70% probability
    RandAffined( # Best Avg. Dice: 0.8801067664816573
        keys=["image", "label"],
        rotate_range=[(degree_min,degree_max),(degree_min,degree_max),(degree_min,degree_max)], # rotation around all 3 axes
        mode=("bilinear", "nearest"),
        padding_mode=("zeros"), # pad with zeros outside original bounds
        prob=0.70,
    ),
    ToTensord(keys=["image", "label"]), # convert image and label to PyTorch tensors
])

# define validation data transformations (no augmentation)
val_transforms = Compose([
    # load NIfTI images and labels from file paths into dictionaries with "image" and "label" keys
    LoadImaged(keys=["image", "label"]),
    # ensures the input image and label have the channel as the first dimension: [C, H, W, D]
    EnsureChannelFirstd(keys=["image", "label"]),
    Lambdad(keys="label", func=combine_fetus_labels),
    # Resample both image and label to a common voxel spacing: 0.8 mm x 0.8 mm x 4.0 mm
    Spacingd(keys=["image", "label"], pixdim=(0.8, 0.8, 4.0), mode=("bilinear", "nearest")),
    # symmetrically pad images and labels to reach the `target_size` before resizing
    SpatialPadd(keys=["image", "label"], spatial_size=target_size, method="symmetric"),
    # resize both image and label to the target 3D shape
    Resized(keys=["image", "label"], spatial_size=target_size, mode=("trilinear", "nearest")),
    # normalize the intensity values of the image to [0, 1] or similar range
    ScaleIntensityd(keys=["image"]),
    ToTensord(keys=["image", "label"]), # convert image and label to PyTorch tensors
])

"""If output is [0, 1] → Binary segmentation (foreground vs background)
- Set: lab_num = 1, class_num = 2

If output is [0, 1, 2, 3] → Multi-class segmentation
- Set: lab_num = 3, class_num = 4

Segmentation masks contain the values:
[0, 1, 2, 3, 4, 5]

This means:
- 0 → Background
- 1, 2, 3, 4, 5 → 5 foreground classes

Setting up the data pipeline for training a fetal MRI segmentation model using MONAI.

*   Transforms: loads and formats the images, resamples to a standard voxel size, pad and resize them to a fixed shape, normalize intensity, and applies random 3D rotations during training for data augmentation.
*   Labels: simplifies segmentation task by combining related fetal structures (like head and body) into one label.
*   Datasets: CacheDataset speeds things up by keeping preprocessed data in memory.
  * data: list of dictionaries with "image" and "label" paths
  * transform: preprocessing and augmentation pipeline
  * cache_num: number of items to cache in memory (speeds up training)
  * cache_rate: 1.0 means cache all items (useful if memory allows)
  * num_workers: number of parallel threads to load data
*   DataLoaders: handles batching and loading the data — with shuffling for training and fixed order for validation.
  * batch_size = 1 (you can increase if you have enough GPU memory)
  * shuffle = True to randomize the batches
  * pin_memory = True improves GPU data transfer speed

This ensures consistent, fast, and augmented data flow to the model.
"""

# Create datasets and dataloaders
train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_num=100, cache_rate=1.0, num_workers=8)
val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=6, cache_rate=1.0, num_workers=4)

train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)

# Check transformed shapes
def check_transformed_shapes(dataloader, dataset_name):
    print(f"Checking {dataset_name} dataset shapes after transformation:")
    for i, batch in enumerate(dataloader):
        image, label = batch["image"], batch["label"]
        print(f"Sample {i}: Image shape {image.shape}, Label shape {label.shape}")
        if i == 4:
            break  # Check first 5 samples

check_transformed_shapes(train_loader, "Training")
check_transformed_shapes(val_loader, "Validation")

"""Visualise validation sets"""

s=round(128/2)

for x in range(len(val_files)):

  case_num = x
  img_name = val_files[case_num]["image"]
  label_name = val_files[case_num]["label"]
  case_in=val_ds[case_num]
  img = case_in["image"]
  label = case_in["label"]
  img_shape = img.shape
  label_shape = label.shape

  print(x, img_name, img_shape, label_shape)
  plt.figure("image", (20, 50))
  plt.subplot(1, 6, 1)
  plt.imshow(img[0, :, :, s], cmap="gray")
  plt.subplot(1, 6, 2)
  plt.imshow(label[0, :, :, s], cmap="jet")
  plt.subplot(1, 6, 3)
  plt.imshow(img[0, :, s, :], cmap="gray")
  plt.subplot(1, 6, 4)
  plt.imshow(label[0, :, s, :], cmap="jet")
  plt.subplot(1, 6, 5)
  plt.imshow(img[0, s, :, :], cmap="gray")
  plt.subplot(1, 6, 6)
  plt.imshow(label[0, s, :, :], cmap="jet")
  plt.show()

"""# Training

Define the network using MONAI
"""

from torch.optim.lr_scheduler import ReduceLROnPlateau

# CUDA device order for consistent GPU usage across platforms
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

# set the device to GPU if available, else fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3D U-Net model
model = UNet(
    spatial_dims=3,          # 3D volumes as input
    in_channels=1,           # input has 1 channel (grayscale MRI)
    out_channels=5,          # 5 output classes (fetus, placenta, cord, fluid, background)
    channels=(32, 64, 128, 256, 512),  # feature map channels at each level
    strides=(2, 2, 2, 2),     # downsampling by 2 at each level
    kernel_size=3,           # 3x3x3 convolution kernels
    up_kernel_size=3,        # 3x3x3 upsampling kernels
    num_res_units=1,         # 1 residual unit per layer
    act='PRELU',             # parametric ReLU activation
    norm='INSTANCE',         # instance normalization (better for medical imaging)
    dropout=0.5              # dropout to reduce overfitting
).to(device)



# loss: combined dice and cross-entropy loss
# - to_onehot_y=True converts labels to one-hot encoding
# - softmax=True applies softmax to model outputs
# - lambda_dice and lambda_ce balance the two loss terms
loss_function = DiceCELoss(to_onehot_y=True, softmax=True, lambda_dice=0.7, lambda_ce=0.3)
torch.backends.cudnn.benchmark = True

# AdamW optimizer with weight decay for regularization
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,               # learning rate
    weight_decay=1e-4      # regularization to reduce overfitting
)

"""This section defines the full training and evaluation loop for a 3D U-Net segmentation model on fetal MRI data.

Training (train): runs a full training epoch:



*   loads batches of images and labels
*   computes Dice + Cross Entropy loss
*   backpropagation and updates weights
*   every eval_num steps: runs validation to compute average Dice score + saves the model if performance improves
*   every 100 steps: displays segmentation predictions overlaid on MRI slices using show_image

Validation (validation)
*   evaluates model predictions using sliding window inference.
*   post-processing and computes the mean Dice score across the validation set.

Visualization (show_image)
*   3 orthogonal views (axial, sagittal, coronal) of the predicted segmentation overlaid on grayscale MRI.
"""

qq=64
alpha_val = 0.4
def show_image(img, logit_map, qq):

    plt.figure("cnn output", (12, 36))

    plt.subplot(1, 3, 1)
    plt.imshow(img.cpu().numpy()[0, 0, qq, :, :], cmap="gray")
    plt.imshow(torch.argmax(logit_map, dim=1).detach().cpu().numpy()[0, qq, :, :], alpha=alpha_val, vmin=0, vmax=5, cmap="jet")

    plt.subplot(1, 3, 2)
    plt.imshow(img.cpu().numpy()[0, 0, :, qq, :], cmap="gray")
    plt.imshow(torch.argmax(logit_map, dim=1).detach().cpu().numpy()[0, :, qq, :], alpha=alpha_val, vmin=0, vmax=5, cmap="jet")

    plt.subplot(1, 3, 3)
    plt.imshow(img.cpu().numpy()[0, 0, :, :, qq], cmap="gray")
    plt.imshow(torch.argmax(logit_map, dim=1).detach().cpu().numpy()[0, :, :, qq], alpha=alpha_val, vmin=0, vmax=5, cmap="jet")

    plt.show()

img_dim = 128
def validation(epoch_iterator_val):
    model.eval()
    dice_vals = list()
    with torch.no_grad():
        for step, batch in enumerate(epoch_iterator_val):
            val_inputs, val_labels = (batch["image"].cuda(), batch["label"].cuda())
            val_outputs = sliding_window_inference(val_inputs, (img_dim, img_dim, img_dim), 4, model)
            val_labels_list = decollate_batch(val_labels)
            val_labels_convert = [
                post_label(val_label_tensor) for val_label_tensor in val_labels_list
            ]
            val_outputs_list = decollate_batch(val_outputs)
            val_output_convert = [
                post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list
            ]
            dice_metric(y_pred=val_output_convert, y=val_labels_convert)
            dice = dice_metric.aggregate().item()
            dice_vals.append(dice)
            epoch_iterator_val.set_description(
                "Validate (%d / %d Steps) (dice=%2.5f)" % (global_step, 10.0, dice)
            )
        dice_metric.reset()
    mean_dice_val = np.mean(dice_vals)
    return mean_dice_val


def train(global_step, train_loader, dice_val_best, global_step_best):
    model.train()
    epoch_loss = 0
    step = 0
    epoch_iterator = tqdm(
        train_loader, desc="Training (X / X Steps) (loss=X.X)", dynamic_ncols=True
    )
    for step, batch in enumerate(epoch_iterator):
        step += 1
        x, y = (batch["image"].cuda(), batch["label"].cuda())
        logit_map = model(x)
        loss = loss_function(logit_map, y)
        loss.backward()
        epoch_loss += loss.item()
        optimizer.step()
        optimizer.zero_grad()
        epoch_iterator.set_description(
            "Training (%d / %d Steps) (loss=%2.5f)" % (global_step, max_iterations, loss)
        )
        if (
            global_step % eval_num == 0 and global_step != 0
        ) or global_step == max_iterations:
            epoch_iterator_val = tqdm(
                val_loader, desc="Validate (X / X Steps) (dice=X.X)", dynamic_ncols=True
            )
            dice_val = validation(epoch_iterator_val)
            epoch_loss /= step
            epoch_loss_values.append(epoch_loss)
            metric_values.append(dice_val)
            if dice_val > dice_val_best:
                dice_val_best = dice_val
                global_step_best = global_step
                torch.save(
                    model.state_dict(), os.path.join(data_dir, "3d_best_metric_model.pth")
                )
                print(
                    "Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}".format(
                        dice_val_best, dice_val
                    )
                )
            else:
                torch.save(
                    model.state_dict(), os.path.join(data_dir, "3d_latest_metric_model.pth")
                )
                print(
                    "Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}".format(
                        dice_val_best, dice_val
                    )
                )
        if global_step % 100 == 0 and global_step != 0:
                show_image(x,model(x),qq)  # showing result of best model

        global_step += 1
    return global_step, dice_val_best, global_step_best

max_iterations = 8000
eval_num = 200
post_label = AsDiscrete(to_onehot=5)
post_pred = AsDiscrete(argmax=True, to_onehot=5)
dice_metric = DiceMetric(include_background=True, reduction="mean", get_not_nans=False)
global_step = 0
dice_val_best = 0.0
global_step_best = 0
epoch_loss_values = []
metric_values = []

while global_step < max_iterations:
    global_step, dice_val_best, global_step_best = train(
        global_step, train_loader, dice_val_best, global_step_best
    )

# model.load_state_dict(torch.load(os.path.join(data_dir,"best_metric_model.pth")))
# model.eval()

# model.load_state_dict(torch.load(os.path.join(data_dir,"latest_metric_model.pth")))
# model.eval()

plt.figure("train", (12, 6))
plt.subplot(1, 2, 1)
plt.title("Iteration Average Loss")
x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]
y = epoch_loss_values
plt.xlabel("Iteration")
plt.plot(x, y)
plt.subplot(1, 2, 2)
plt.title("Val Mean Dice")
x = [eval_num * (i + 1) for i in range(len(metric_values))]
y = metric_values
plt.xlabel("Iteration")
plt.plot(x, y)
plt.show()

"""Loads latest model"""

model.load_state_dict(torch.load(os.path.join(data_dir, "3d_latest_metric_model.pth")))
model.eval()

"""Selects a single validation MRI case and performs inference using the trained 3D U-Net model with sliding window inference.

Visualizes the results across three anatomical planes (axial, sagittal, coronal) by overlaying the predicted segmentation and ground truth labels on the grayscale input image.

Visually inspect how well the model's output aligns with the actual labels.
"""

case_num = 1

class_n = 6
qq=round(img_dim/2)
alpha_val = 0.6
with torch.no_grad():
    img_name = val_files[case_num]["image"]
    cur_case = val_ds[case_num]
    img = cur_case["image"]
    label = cur_case["label"]
    val_inputs = torch.unsqueeze(img, 1).cuda()
    val_labels = torch.unsqueeze(label, 1).cuda()
    val_outputs = sliding_window_inference(
        val_inputs, (img_dim, img_dim, img_dim), 4, model, overlap=0.8
    )
    plt.figure("check", (18, 18))
    plt.subplot(3, 3, 1)
    plt.title("Input image")
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, qq], cmap="gray")
    plt.subplot(3, 3, 2)
    plt.title("GT label")
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, qq], cmap="gray")
    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, qq], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")
    plt.subplot(3, 3, 3)
    plt.title("CNN output")
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, qq], cmap="gray")
    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, qq], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")

    plt.subplot(3, 3, 4)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, qq, :], cmap="gray")
    plt.subplot(3, 3, 5)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, qq, :], cmap="gray")
    plt.imshow(val_labels.cpu().numpy()[0, 0, :, qq, :], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")
    plt.subplot(3, 3, 6)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, qq, :], cmap="gray")
    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, qq, :], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")


    plt.subplot(3, 3, 7)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, qq, :, :], cmap="gray")
    plt.subplot(3, 3, 8)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, qq, :, :], cmap="gray")
    plt.imshow(val_labels.cpu().numpy()[0, 0, qq, :, :], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")
    plt.subplot(3, 3, 9)
    plt.imshow(val_inputs.cpu().numpy()[0, 0, qq, :, :], cmap="gray")
    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, qq, :, :], alpha=alpha_val, vmin=0, vmax=class_n, cmap="jet")

    plt.show()

"""Save CNN segmentation results for all images"""

model.load_state_dict(torch.load(os.path.join(data_dir, "3d_best_metric_model.pth")))
model.eval()

import os
import nibabel as nib
import torch
import torch.nn.functional as F

res_dir = data_dir + "3d_results"
os.makedirs(res_dir, exist_ok=True)

for x in range(len(val_files)):
    case_num = x
    img_name = val_files[case_num]["image"]
    label_name = val_files[case_num]["label"]
    case_name = os.path.basename(img_name)
    out_name = os.path.join(res_dir, f"3d-cnn-lab-{case_name}")

    print(case_num, out_name)

    # Load original image to get affine and header
    img_tmp_info = nib.load(img_name)
    original_shape = img_tmp_info.shape  # (H, W, D)

    with torch.no_grad():
        img = val_ds[case_num]["image"]
        val_inputs = torch.unsqueeze(img, 1).cuda()  # (B, C, H, W, D)

        val_outputs = sliding_window_inference(
            val_inputs, (img_dim, img_dim, img_dim), sw_batch_size=4, predictor=model, overlap=0.8
        )

        out_label = torch.argmax(val_outputs, dim=1).detach().cpu().squeeze(0)  # (H, W, D)

        # --- Resize prediction back to original shape ---
        out_label = out_label.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W, D] for interpolate
        out_label_resized = F.interpolate(
            out_label.float(), size=original_shape, mode="nearest"
        ).squeeze().byte()  # After resize, remove batch/channel dims

        # --- Save ---
        out_lab_nii = nib.Nifti1Image(out_label_resized.numpy(), img_tmp_info.affine, img_tmp_info.header)
        nib.save(out_lab_nii, out_name)