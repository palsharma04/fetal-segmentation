# -*- coding: utf-8 -*-
"""all-nnUNet_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UFs88lmYlpep2Ky0KF6HgeB6D6mH1_hq
"""

from google.colab import drive
drive.mount('/content/drive/')

"""nnU-Net relies on environment variables to know where raw data, preprocessed data and trained model weights are stored. To use the full functionality of nnU-Net, the following three environment variables must be set:

- **nnUNet_raw**: This is where you place the raw datasets. This folder will have one subfolder for each dataset names DatasetXXX_YYY where XXX is a 3-digit identifier (such as 001, 002, 043, 999, ...) and YYY is the (unique) dataset name.

- **nnUNet_preprocessed**: This is the folder where the preprocessed data will be saved. The data will also be read from this folder during training. It is important that this folder is located on a drive with low access latency and high throughput (such as a nvme SSD (PCIe gen 3 is sufficient)).

- **nnUNet_results**: This specifies where nnU-Net will save the model weights. If pretrained models are downloaded, this is where it will save them.
"""

# Install nnUNet
!pip install --quiet nnunetv2

# Set environment variables
import os

os.environ['nnUNet_raw'] = '/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw'
os.environ['nnUNet_preprocessed'] = '/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_preprocessed'
os.environ['nnUNet_results'] = '/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_results'


# Verify the environment setup
!echo "nnUNet_raw: $nnUNet_raw"
!echo "nnUNet_preprocessed: $nnUNet_preprocessed"
!echo "nnUNet_results: $nnUNet_results"

"""# Pre-processing

So far nnU-Net only supports supervised pre-training, meaning that you train a regular nnU-Net on some pretraining dataset and then use the final network weights as initialization for your target dataset.

As a reminder, many training hyperparameters such as patch size and network topology differ between datasets as a result of the automated dataset analysis and experiment planning nnU-Net is known for. So, out of the box, it is not possible to simply take the network weights from some dataset and then reuse them for another.

- pretraining dataset is the dataset you intend to run the pretraining on
- finetuning dataset is the dataset you are interested in; the one you wish to fine tune on

Given a new dataset, nnU-Net will extract a dataset fingerprint (a set of dataset-specific properties such as image sizes, voxel spacings, intensity information etc). This information is used to design three U-Net configurations. Each of these pipelines operates on its own preprocessed version of the dataset.

The easiest way to run fingerprint extraction, experiment planning and preprocessing is to use:

nnUNetv2_plan_and_preprocess -d DATASET_ID --verify_dataset_integrity
Where DATASET_ID is the dataset id (duh). We recommend --verify_dataset_integrity whenever it's the first time you run this command. This will check for some of the most common error sources!

You can also process several datasets at once by giving -d 1 2 3 [...]. If you already know what U-Net configuration you need you can also specify that with -c 3d_fullres (make sure to adapt -np in this case!). For more information about all the options available to you please run nnUNetv2_plan_and_preprocess -h.

nnUNetv2_plan_and_preprocess will create a new subfolder in your nnUNet_preprocessed folder named after the dataset. Once the command is completed there will be a dataset_fingerprint.json file as well as a nnUNetPlans.json file for you to look at (in case you are interested!). There will also be subfolders containing the preprocessed data for your UNet configurations.
"""

# Step 1: plan and precess dataset
# !nnUNetv2_plan_and_preprocess -d 001 --verify_dataset_integrity
# I've seperated them to focus on the error for the 3d_lowres - background workers keep dying so I may
# have to edit its patch_size and spacing in the nnUNet_plans.json file
! nnUNetv2_extract_fingerprint -d 001

!nnUNetv2_plan_experiment -d 001

"""The idea is to speed up the preprocessing by dividing the work among these processes (increasing the number of workers). However, as you've seen, using multiple processes can also lead to increased memory usage.

Had to change the number of workers to 1.
"""

# configurations: 2d, 3d_fullres, 3d_lowres
!nnUNetv2_preprocess -d 001

"""# Training

You pick which configurations (2d, 3d_fullres, 3d_lowres, 3d_cascade_fullres) should be trained! If you have no idea what performs best on your data, just run all of them and let nnU-Net identify the best one. It's up to you!

nnU-Net trains all configurations in a 5-fold cross-validation over the training cases. This is 1) needed so that nnU-Net can estimate the performance of each configuration and tell you which one should be used for your segmentation problem and 2) a natural way of obtaining a good model ensemble (average the output of these 5 models for prediction) to boost performance.

You can influence the splits nnU-Net uses for 5-fold cross-validation (see here). If you prefer to train a single model on all training cases, this is also possible (see below).

Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the U-Net cascade (and with it the 3d_lowres configuration) is omitted because the patch size of the full resolution U-Net already covers a large part of the input images.

Training models is done with the nnUNetv2_train command. The general structure of the command is:

nnUNetv2_train DATASET_NAME_OR_ID UNET_CONFIGURATION FOLD [additional options, see -h]
UNET_CONFIGURATION is a string that identifies the requested U-Net configuration (defaults: 2d, 3d_fullres, 3d_lowres, 3d_cascade_lowres). DATASET_NAME_OR_ID specifies what dataset should be trained on and FOLD specifies which fold of the 5-fold-cross-validation is trained.

nnU-Net stores a checkpoint every 50 epochs. If you need to continue a previous training, just add a --c to the training command.

2D
"""

# Step 2: train the model
# changed the num_epochs = 81 and initial_lr = 1e-3 in 'nnUNetTrainer.py'
!nnUNetv2_train 001 2d 0 --npz
!nnUNetv2_train 001 2d 1 --npz
!nnUNetv2_train 001 2d 2 --npz
!nnUNetv2_train 001 2d 3 --npz
!nnUNetv2_train 001 2d 4 --npz

"""3D FULLRES"""

!nnUNetv2_train 001 3d_fullres 0 --npz
!nnUNetv2_train 001 3d_fullres 1 --npz
!nnUNetv2_train 001 3d_fullres 2 --npz
!nnUNetv2_train 001 3d_fullres 3 --npz
!nnUNetv2_train 001 3d_fullres 4 --npz

"""3D LOWRES

I disabled Blosc2 conpression for this step as it wasn't working. In nnunet_dataset.py I replaced "blosc2.asarray(seg, urlpath=output_filename_truncated + '.b2nd', chunks=chunks_seg, blocks=blocks_seg)" with "np.save(output_filename_truncated + '.npy', seg)" in save_seg() and saves everything as .npy instead.

I wrote "print(f"⚠️ Saving segmentation for: {output_file}")" at the bottom of export_prediction.py
"""

!nnUNetv2_train 001 3d_lowres 0 --npz
!nnUNetv2_train 001 3d_lowres 1 --npz
!nnUNetv2_train 001 3d_lowres 2 --npz
!nnUNetv2_train 001 3d_lowres 3 --npz
!nnUNetv2_train 001 3d_lowres 4 --npz

"""Converting the .npy to .bsnd wasn't working so I just renamed them instead of actually compressing the files."""

import os

seg_folder = "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_results/Dataset001_mibirth/nnUNetTrainer__nnUNetPlans__3d_lowres/predicted_next_stage/3d_cascade_fullres"

# Find all .npy files
npy_files = [f for f in os.listdir(seg_folder) if f.endswith(".npy")]

for npy_file in npy_files:
    npy_path = os.path.join(seg_folder, npy_file)
    b2nd_path = npy_path.replace(".npy", ".b2nd")  # Change extension

    # Rename file
    os.rename(npy_path, b2nd_path)
    print(f"Renamed {npy_path} → {b2nd_path}")

print("All .npy files have been renamed to .b2nd!")

"""These are the Dice scores before refining labels(higher is better):

- 2d -(nnUNetTrainer__nnUNetPlans__2d) =	0.902
- 3d_lowres (nnUNetTrainer__nnUNetPlans__3d_lowres)	= 0.927
- 3d_fullres (nnUNetTrainer__nnUNetPlans__3d_fullres)	= 0.927
- Ensemble (2D + 3D lowres)	= 0.923
- Ensemble (2D + 3D fullres)	= 0.921
- Ensemble (3D lowres + 3D fullres) = 0.931 (Best model!)

The 3D models perform better than the 2D model.
The best result is an ensemble of 3d_lowres and 3d_fullres, with a Dice score of 0.937.
Ensembling helped improve performance over individual models.
"""

!nnUNetv2_find_best_configuration 001 -c 2d 3d_lowres 3d_fullres

"""# Testing"""

!nnUNetv2_predict -d Dataset001_mibirth -i "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/imagesTs/" -o "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_2d/" -f  0 1 2 3 4 -tr nnUNetTrainer -c 2d -p nnUNetPlans --save_probabilities

"""nnU-Net checked if removing unwanted small components (artifacts) from the segmentation improves results:

Keeping only the largest region improved accuracy.
Some components were removed because they hurt performance.
Final Dice score after postprocessing: 0.937 (same as before, but better refined).
"""

# Step 3: prediction after training
!nnUNetv2_predict -d Dataset001_mibirth -i "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/imagesTs/" -o "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_3d_lowres/" -f  0 1 2 3 4 -tr nnUNetTrainer -c 3d_lowres -p nnUNetPlans --save_probabilities

!nnUNetv2_predict -d Dataset001_mibirth -i "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/imagesTs/" -o "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_3d_fullres/" -f  0 1 2 3 4 -tr nnUNetTrainer -c 3d_fullres -p nnUNetPlans --save_probabilities

"""This averages the predictions from 3d_lowres and 3d_fullres to get the best result.

-np 8 means it will use 8 CPU cores (adjust based on Colab resources).

"""

!nnUNetv2_ensemble -i "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_3d_lowres/" "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_3d_fullres/" -o "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_ensemble/" -np 8

"""POST PROCESSING

- Removes any small incorrect predictions.

- Keeps the best segmentation.

- The final segmentations are saved in OUTPUT_FOLDER_PP.
"""

!nnUNetv2_apply_postprocessing -i "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_raw/Dataset001_mibirth/labelsTs/predTs_ensemble/" -o "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/post_processing/" -pp_pkl_file "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_results/Dataset001_mibirth/ensembles/ensemble___nnUNetTrainer__nnUNetPlans__3d_lowres___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4/postprocessing.pkl" -np 8 -plans_json "/content/drive/MyDrive/BEng/Year3/Research_Project/nnUNet_training/mibirth/nnUNet_results/Dataset001_mibirth/ensembles/ensemble___nnUNetTrainer__nnUNetPlans__3d_lowres___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4/plans.json"